{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a basic image classification problem where we are supposed to train a supervised machine learning model to identify if an image has a dog in it. We will have a number of labelled data for training, if dog is present labelled as 1 and if dog is absent labelled as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/1.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our target varibale, y, will have two options 0 and 1. Now consider each image is 64 by 64 pixels in size. Since \"Red\", \"Green\", and \"Blue\" are the basic colors and with their combination we can generate other colors we will have 64*64*3 pixel information. We will include all this information in a 12288 by 1 vector *x* (64*64*3=12288). Say, we have \"m\" such pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/2.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Linear vs Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification problems don't work well with linear regression algorithm since the training data is of categorical nature. In addition, values predicted by linear regression may be outside the bounds of possible values. A logistic regression, on the other hand, works much better with binomial classification problem. It is built for predicting categorical values and values are always within bound of possible values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/4.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Approach: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of model training process we want a model f(*x*) that maps a *x* not used for training to an appropriate y_hat. When using logistic regression the function is σ(*W.T*+b). As in case of linear regression, we need to determine the appropriate values of slope and intercept for our model, in case of logistic regression we need *w.T* and b. The sigmoid function, σ(x), (s-curve) is already defined as 1/(1+*e*^-x). Understand that *w* needs to be a vector of same dimension as *x*. When *w.T* and *x* are multiplied we get a real number and add b to it. Finally, calculate sigmoid of the resulting number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/3.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine *w.T* and b in such a way that for a *x* our predicted value, y_hat, and actual value, *y*, are same for most of the inputs. To do that, we will need a cost/loss function, whose value we need to minimize. *J(w,b)* is the average of costs obtained using cost function *L(y_hat, y)* for m data points. Cost function for one data point, L, can be a number of functions involving y_hat and *y*. We can define out own cost functions as well. It can be as simple as |y_hat-y|. But for machine learning algorithms we use a cost function for which average cost when plotted against estimated parameters (here, w.T and b) has a global minima. For logistic regression, binary cross entropy (shown below) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/5.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said earlier, for machine learning, We use a cost function for which average cost when plotted against estimated parameters has a global minima (bowl shape). The idea is that we want to estimate the paramteres *w.T and b* such that the average cost is minimum. Although, *w* has a very high dimention, since we cannot plot that many dimensions, imagine it is one dimentional. Notice the convex curve below obtained by ploting w against average cost based.\n",
    "\n",
    "Now, to find the maxima/minima, mathematical procedure would be to calculate first derivative of *J(w)* with respect to w and equate it to zero. However, since w is a high demensional vector, it gets diccicult to solve it analytically. Instead of trying to calcute exact value of w, we can estimate it. May not be the exact value, but good enough for the task at hand with less computational expense. The algorithm used for estimating value of parameters in order to minimize average cost is known as Gradient Descent Algorithm. It is also known as Hill climbing algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/6.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure below, the left of minimum point has a negative gradient and the right of minimum point has a positive gradient. We start with a random value of *w*. Using that value calculate *J(w)*. Next, we calcuate first derivative at that point. If the slope is negative, we need a lower *w*, if it is positive, we need a higher value for *w*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/7.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process can be repeted until convergence. Note that w is not increased/decreased randomly. It is increased or decreased by  α times first derivative of *J* at previous point. The value α is known as learning rate and its value depends upon the work at hand. We use hyperparameter tuning along with domain knowledge for finding appropriate α."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/8.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that α can neither be too high or too low. If α is too high we may keep missing the minimum point, if α is too low, the process may take a lot of time for convergence. Either of them for a high dimensional vector *w* leads to high computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/9.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we followed for *w* is parallelly adopted for estimating *b*. But since *b* is just a simple number its impact on the process is not so much. Finally, we use the estimated *w* and *b* to calculate y_hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Summary: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize the steps we followed :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ujwal-sah/blogs/master/Introduction%20to%20Deep%20Neural%20Networks/10.png\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational Graphs and Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
